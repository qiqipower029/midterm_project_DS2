---
title: "Linear classification and trees"
author: "Jieqi Tu (jt3098)"
date: "5/13/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(AppliedPredictiveModeling)
library(pROC)
```


#Load and tidy data
```{r}
#read data
rawdata <- read.csv("kc_house_data.csv", header = TRUE)

#inspect the structure of data
str(rawdata)
```


```{r}
#clean the rawdata; create a tidied dataset for analysis and modelling.
#subset data: only those with view >0 and basement >0.
housing = 
  rawdata %>% 
  select(-id, -date, -zipcode, -lat, -long) %>% 
  filter(view > 0, bedrooms <30) %>% 
  mutate(basement = ifelse(sqft_basement == 0, 0, 1),
         renovated = ifelse(yr_renovated == 0, 0, 1)) %>% 
  filter(basement > 0) %>% 
  select(-sqft_basement, -yr_renovated, -view, - sqft_living, -sqft_lot, -basement) 

# dichotimize response variable
median(housing$price) #805000
housing <- housing %>% mutate(price.new = ifelse(price>805000, "High", "Low")) 
housing$price.new <- factor(housing$price.new, c("High", "Low"))
# create training data and testing data.
rowTrain <- createDataPartition(y = housing$price, 
                                p=0.8, list = FALSE)
```

# Data visuailization
```{r data visualization for linear classification}
transparentTheme(trans = 0.4)
featurePlot(x = housing[,2:12],
            y = housing$price.new,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            auto.key = list(columns = 2))
```

From the feature plot we could see that, the two classes of price are distributed differently in features. 

# Logistic Regression
```{r logistic regression}
# Use caret package
ctrl = trainControl(method = "cv",
                    summaryFunction = twoClassSummary,
                    classProbs = T)
set.seed(1)
model.glm = train(x = housing[rowTrain, 2:12],
                  y = housing$price.new[rowTrain],
                  method = "glm",
                  metric = "ROC",
                  trControl = ctrl)
# test performance
pred.glm = predict(model.glm, newdata = housing[-rowTrain,2:13], response = "prob")
pred.glm.prob = predict(model.glm, newdata = housing[-rowTrain,2:13], type = "prob")
confusionMatrix(data = as.factor(pred.glm), reference = housing$price.new[-rowTrain])

# plot the ROC curve
roc.glm = roc(housing$price.new[-rowTrain], pred.glm.prob$High)
plot(roc.glm, legacy.axes = T, print.auc = T)
plot(smooth(roc.glm), col = 4, add = T)
```

The AUC is 0.943. The accuracy is 0.8676. Sensitivity is 0.8042, Specificity is 0.9306.

# Discriminant Analysis
```{r LDA using caret}
library(MASS)
set.seed(1)
model.lda = train(x = housing[rowTrain, 2:12],
                  y = housing$price.new[rowTrain],
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)
pred.lda = predict(model.lda, newdata = housing[-rowTrain,2:13], type = "prob")
roc.lda = roc(housing$price.new[-rowTrain], pred.lda$High, levels = c("High", "Low"))
plot(roc.lda, legacy.axes = T, print.auc = T)
```

The AUC for LDA is 0.933.

```{r QDA using caret package}
set.seed(1)
model.qda = train(x = housing[rowTrain, 2:12],
                  y = housing$price.new[rowTrain],
                  method = "qda",
                  metric = "ROC",
                  trControl = ctrl)

pred.qda = predict(model.qda, newdata = housing[-rowTrain, 2:13], type = "prob")
roc.qda = roc(housing$price.new[-rowTrain], pred.qda$High, levels = c("High", "Low"))
plot(roc.qda, legacy.axes = T, print.auc = T)
```

The AUC for QDA is 0.889.

```{r KNN using caret}
set.seed(1)
model.knn = train(x = housing[rowTrain, 2:12],
                  y = housing$price.new[rowTrain],
                  method = "knn",
                  preProcess = c("center", "scale"),
                  metric = "ROC",
                  tuneGrid = data.frame(k = seq(1, 200, by = 5)),
                  trControl = ctrl)
ggplot(model.knn) + theme_bw()
```

```{r Naive Bayes using caret}
set.seed(1)
nbGrid = expand.grid(usekernel = c(F, T),
                     fL = 1,
                     adjust = seq(0, 5, by = 1))
model.nb = train(x = housing[rowTrain, 2:12],
                 y = housing$price.new[rowTrain],
                 method = "nb",
                 metric = "ROC",
                 tuneGrid = nbGrid,
                 trControl = ctrl)
plot(model.nb)
```

### Model Comparism
```{r compare models}
res = resamples(list(GLM = model.glm, LDA = model.lda, QDA = model.qda, NB = model.nb, KNN = model.knn))

summary(res)
```

GLM and LDA tend to have higher AUC values.
