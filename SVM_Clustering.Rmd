---
title: "SVM and Clustering"
author: "Jiayi Shen"
date: "5/11/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret) 
library(e1071)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer) 
library(gplots)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(randomForest)
library(ranger)
library(gbm)
library(plotmo)
library(pdp)
library(pROC)
library(lime)
```

#Load and tidy data
```{r}
#read data
rawdata <- read.csv("kc_house_data.csv", header = TRUE)

#inspect the structure of data
str(rawdata)
```


```{r}
#clean the rawdata; create a tidied dataset for analysis and modelling.
#subset data: only those with view >0 and basement >0.
housing = 
  rawdata %>% 
  select(-id, -date, -zipcode, -lat, -long) %>% 
  filter(view > 0, bedrooms <30) %>% 
  mutate(basement = ifelse(sqft_basement == 0, 0, 1),
         renovated = ifelse(yr_renovated == 0, 0, 1)) %>% 
  filter(basement > 0) %>% 
  select(-sqft_basement, -yr_renovated, -view, - sqft_living, -sqft_lot, -basement) 

# dichotimize response variable
 median(housing$price) #= 805000
housing <- housing %>% 
  mutate(price.new = ifelse(price>805000, "High", "Low")) %>% 
  select(-price) 
housing$price.new <- factor(housing$price.new, c("High", "Low"))
# create training data and testing data.
rowTrain <- createDataPartition(y = housing$price.new, 
                                p=0.8, list = FALSE)
```

# SVM
Using `caret`  
Linear Kernel
```{r Linear Kernel}
ctrl <- trainControl(method = "cv")

set.seed(1)
svml.fit <- train(price.new~., 
                  data = housing[rowTrain,], 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-4,0,len=25))),
                  trControl = ctrl)

ggplot(svml.fit, highlight = TRUE)
svml.fit$bestTune$cost #0.311
```

Radial Kernel
```{r Radial Kernel}
svmr.grid <- expand.grid(C = exp(seq(1,5,len=10)),
                         sigma = exp(seq(-8,-3,len=5)))
set.seed(1)             
svmr.fit <- train(price.new~., 
                  data = housing, 
                  subset = rowTrain,
                  method = "svmRadial",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

ggplot(svmr.fit, highlight = TRUE)
svmr.fit$bestTune #sigma=0.004;  C=95
```


```{r resamples}
resamp <- resamples(list(svmr = svmr.fit, svml = svml.fit))
bwplot(resamp)
```

### Test data performance for SVM
```{r}
pred.svml <- predict(svml.fit, newdata = housing[-rowTrain,])
pred.svmr <- predict(svmr.fit, newdata = housing[-rowTrain,])

confusionMatrix(data = pred.svml, 
                reference = housing$price.new[-rowTrain]) #0.8229

confusionMatrix(data = pred.svmr, 
                reference = housing$price.new[-rowTrain]) #0.8368 
```

# Clustering
## K-means
```{r}
housing1 <- housing[,1:11]
housing1 <- scale(housing1)

fviz_nbclust(housing1,
             FUNcluster = kmeans,
             method = "silhouette")
#optimal number of clusters = 2
```

```{r}
set.seed(1)
km <- kmeans(housing1, centers = 2, nstart = 20)
km_vis <- fviz_cluster(list(data = housing1, cluster = km$cluster), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 

km_vis
```

## Hierarchical clustering
```{r, include = FALSE}
hc.complete <- hclust(dist(housing1), method = "complete")
hc.average <- hclust(dist(housing1), method = "average")
hc.single <- hclust(dist(housing1), method = "single")
hc.centroid <- hclust(dist(housing1), method = "centroid")

complete_vis <- fviz_dend(hc.complete, k = 2,        
          cex = 0.3, 
          palette = "jco", 
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)
complete_vis

```


```{r, fig.width = 12, fig.height=7}
#display.brewer.all(n=NULL, type="all", select=NULL, exact.n=TRUE)
col1 <- colorRampPalette(brewer.pal(9, "GnBu"))(100)
col2 <- colorRampPalette(brewer.pal(3, "Spectral"))(2)

# now try to reduce the number of clusters
heatmap.2(t(housing1), 
          col = col1, keysize=.8, key.par = list(cex=.5),
          trace = "none", key = TRUE, cexCol = 0.75, 
          ColSideColors = col2[as.numeric(housing[,"price.new"])+1],
          margins = c(10, 10))
```

# PCA
```{r, fig.height=3}
pca <- prcomp(housing1)
pca$rotation
pca$sdev
pca$rotation %*% diag(pca$sdev)
corrplot(pca$rotation %*% diag(pca$sdev))

var <- get_pca_var(pca)
corrplot(var$cor)
```


```{r, fig.height=4}
#plots the eigenvalues/variances against the number of dimensions. 
fviz_eig(pca, addlabels = TRUE)
```


```{r}
a <- fviz_contrib(pca, choice = "var", axes = 1)
b <- fviz_contrib(pca, choice = "var", axes = 2)
#visualize the contribution of variables from the results of PCA.
grid.arrange(a, b, nrow = 2)
```

`sqrt_above`, `grade`, `bathrooms`, `sqrt_living15`, and `floors` contribute more to the first principal component, compared to other variables. `renovated`, `yr_built`, and `waterfront` contribute more to the second pricipal ccomponent.   

```{r, fig.height=4}
# to obtain the biplot of individuals and variables.
fviz_pca_biplot(pca, axes = c(1,2),
                habillage = housing$price.new,
                label = c("var"),
                addEllipses = TRUE) 

fviz_pca_var(pca, col.var = "steelblue", repel = TRUE)
fviz_pca_ind(pca,
             habillage = housing$price.new,
             label = "none",
             addEllipses = TRUE)
```

With only two principal components, we can distinguish the two classes reasonably well.  

\newpage  
# Classification tree
## CART
```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(1)
rpart.fit <- train(price.new~., housing, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-4, len = 20))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)

# Test set performance
rpart.pred.prob <- predict(rpart.fit, newdata = housing[-rowTrain,], type = "prob")[,1]
rpart.pred <- rep("Low", length(rpart.pred.prob))
rpart.pred[rpart.pred.prob>0.5] <- "High"
confusionMatrix(data = as.factor(rpart.pred),
                reference = housing$price.new[-rowTrain],
                positive = "High")
# 0.8194  
```

## CIT
```{r}
set.seed(1)
ctree.fit <- train(price.new~., housing, 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-5, -2, length = 20))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)

# Test set performance
ctree.pred.prob <- predict(ctree.fit, newdata = housing[-rowTrain,], type = "prob")[,1]
ctree.pred <- rep("Low", length(ctree.pred.prob))
ctree.pred[ctree.pred.prob>0.5] <- "High"
confusionMatrix(data = as.factor(ctree.pred),
                reference = housing$price.new[-rowTrain],
                positive = "High")
# 0.7951


```

## Random Forest
```{r}
rf.grid <- expand.grid(mtry = 1:6,
                       splitrule = "gini",
                       min.node.size = 1:6)
set.seed(1)
rf.fit <- train(price.new~., housing, 
                subset = rowTrain,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl)

ggplot(rf.fit, highlight = TRUE)


# Test set performance
rf.pred.prob <- predict(rf.fit, newdata = housing[-rowTrain,], type = "prob")[,1]
rf.pred <- rep("Low", length(rf.pred.prob))
rf.pred[rf.pred.prob>0.5] <- "High"
confusionMatrix(data = as.factor(rf.pred),
                reference = housing$price.new[-rowTrain],
                positive = "High")
# 0.8056 

# Explain your prediction 
new_obs <- housing[-rowTrain,-12][1:2,]
explainer.rf <- lime(housing[rowTrain,-12], rf.fit)
explanation.rf <- explain(new_obs, explainer.rf, n_features = 8,
                          labels = "High")
plot_features(explanation.rf)
```

The optimal mtry is 2 and the minimal node size picked up by the optimal model is 3.  

### Variable importance of random forest model
```{r}

set.seed(1)
rf2.final.imp <- ranger(price.new~., housing,
                        mtry = 2, splitrule = "gini",
                        min.node.size = 3,
                        importance = "impurity") 

barplot(sort(ranger::importance(rf2.final.imp), decreasing = FALSE), 
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

```

Using node impurity as the measure of varaible importance, the top three important variable are `grade`, `sqft_above`, and `sqft_living15`.    


## Boosting

### Binomial loss

```{r}
gbmB.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:2,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(1)
# Binomial loss function
gbmB.fit <- train(price.new~., housing, 
                 subset = rowTrain, 
                 tuneGrid = gbmB.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "bernoulli",
                 metric = "ROC",
                 verbose = FALSE)

ggplot(gbmB.fit, highlight = TRUE)

# Test set performance
gbmB.pred.prob <- predict(gbmB.fit, newdata = housing[-rowTrain,], type = "prob")[,1]
gbmB.pred <- rep("Low", length(gbmB.pred.prob))
gbmB.pred[gbmB.pred.prob>0.5] <- "High"
confusionMatrix(data = as.factor(gbmB.pred),
                reference = housing$price.new[-rowTrain],
                positive = "High")
# 0.8333 

#Variable importance
summary(gbmB.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

The top three important variable are `grade`, `sqft_above`, and `sqft_living15`.  

### AdaBoost

```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                        interaction.depth = 1:2,
                        shrinkage = c(0.001,0.003,0.005),
                        n.minobsinnode = 1)
set.seed(1)
# Adaboost loss function
gbmA.fit <- train(price.new~., housing, 
                 subset = rowTrain, 
                 tuneGrid = gbmA.grid,
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)

ggplot(gbmA.fit, highlight = TRUE)

# Test set performance
gbmA.pred.prob <- predict(gbmA.fit, newdata = housing[-rowTrain,], type = "prob")[,1]
gbmA.pred <- rep("Low", length(gbmA.pred.prob))
gbmA.pred[gbmA.pred.prob>0.5] <- "High"
confusionMatrix(data = as.factor(gbmA.pred),
                reference = housing$price.new[-rowTrain],
                positive = "High") 
# 0.8333

# Explain your prediction 
new_obs <- housing[-rowTrain,-12][1:2,]
explainer.gbm <- lime(housing[rowTrain,-12], gbmA.fit)
explanation.gbm <- explain(new_obs, explainer.gbm, n_features = 8,
                           labels = "High")
plot_features(explanation.gbm)

#Variable importance
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

The top three important variable are also `grade`, `sqft_above`, and `sqft_living15`.    


### Resamples
```{r}
resamp.tree <- resamples(list(rf = rf.fit, 
                         gbmA = gbmA.fit,
                         gbmB = gbmB.fit,
                         rpart = rpart.fit,
                         ctree = ctree.fit))
bwplot(resamp.tree, metric = "ROC")
```



### Variable importance - PDP
```{r}
pdp.rf <- rf.fit %>% 
  partial(pred.var = "condition", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = housing[rowTrain,]) +
  ggtitle("Random forest") 

pdp.gbm <- gbmA.fit %>% 
  partial(pred.var = "condition", 
          grid.resolution = 100,
          prob = TRUE) %>%
  autoplot(rug = TRUE, train = housing[rowTrain,]) +
  ggtitle("Boosting") 

grid.arrange(pdp.rf, pdp.gbm, nrow = 1)

```

# Test data performance (Tree)

```{r}
roc.rpart <- roc(housing$price.new[-rowTrain], rpart.pred.prob)
roc.ctree <- roc(housing$price.new[-rowTrain], ctree.pred.prob)
roc.rf <- roc(housing$price.new[-rowTrain], rf.pred.prob)
roc.gbmA <- roc(housing$price.new[-rowTrain], gbmA.pred.prob)
roc.gbmB <- roc(housing$price.new[-rowTrain], gbmB.pred.prob)


plot(roc.rpart)
plot(roc.ctree, add = TRUE, col = 2)
plot(roc.rf, add = TRUE, col = 3)
plot(roc.gbmA, add = TRUE, col = 4)
plot(roc.gbmB, add = TRUE, col = 5)


auc <- c(roc.rpart$auc[1],  roc.ctree$auc[1],
         roc.rf$auc[1], roc.gbmA$auc[1], roc.gbmB$auc[1])

modelNames <- c("rpart_caret","ctree","rf","gbmA","gbmB")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```

