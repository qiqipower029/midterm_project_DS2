---
title: "Linear Regression"
author: "Jieqi Tu (jt3098)"
date: "3/26/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnet)
```

### Linear Regression

#Load and tidy data
```{r}
#read data
rawdata <- read.csv("kc_house_data.csv", header = TRUE)

#inspect the structure of data
str(rawdata)
```

  `id`, `date`, `zipcode`, `lat`, `long` can be removed from the dataframe.   
  The cleaned dataset to be used in this project include one response variable `price` and additional 15 variables.  
  `view`, `sqft_basement` and `yr_renovated` are continuous or integer variables in the original dataset. For the purpose of easy interpretation in later modelling, we convert these variables to be binary. Then these variables indicate whether the house has been viewed by potential buyers, whether the house has basement or not and whether the house has been renovated or not, respectively.  

```{r}
#clean the rawdata; create a tidied dataset for analysis and modelling.
housing = 
  rawdata %>% select(-id, -date, -zipcode, -lat, -long) %>% 
  mutate(view = ifelse(view == 0, 0, 1),
         basement = ifelse(sqft_basement == 0, 0, 1),
         renovated = ifelse(yr_renovated == 0, 0, 1)) %>% 
  select(-sqft_basement, -yr_renovated)
```

#Exploratory data analysis
## Continuous variables
```{r}
# vector of response
y <- housing$price

# matrix of continuous predictors 
housing_continous = housing %>% select(c(price, sqft_living, grade, sqft_above, yr_built, sqft_living15, floors, condition, bedrooms, bathrooms, sqft_living, sqft_lot ))
x_continuous <- model.matrix(price~., housing_continous)[,-1]

# set up the theme of plotting
theme1 <- trellis.par.get()   
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

featurePlot(x_continuous, y, plot = "scatter", labels = c("","Y"),
            type = c("p"), layout = c(3, 4))
```

## Binary variables
```{r}
# dataframe of binary predictors 
housing_binary = housing %>% 
  select(c(price,view, waterfront, basement, renovated )) %>%
  mutate(view = as.factor(view),
         waterfront = as.factor(waterfront),
         basement = as.factor(basement),
         renovated = as.factor(renovated)) 

# boxplots
par(mfrow = c(2, 2))
boxplot(price~view, data=housing_binary, ylab = "Price", xlab ="View") 
boxplot(price~waterfront, data=housing_binary, ylab = "Price", xlab ="waterfront") 
boxplot(price~basement, data=housing_binary, ylab = "Price", xlab ="basement") 
boxplot(price~renovated, data=housing_binary, ylab = "Price", xlab ="renovated") 
```

# Model Building
#### Linear model
```{r least square model}
# build a least-square linear model
model_ls = lm(housing_binary$price~., data = housing_binary)
summary(model_ls)
```

```{r ridge regression model}
# prepare the design matrix
x = model.matrix(price~., housing)[,-1]

# build a ridge model using glmnet function
model_ridge = glmnet(x, y, alpha = 0, lambda = exp(seq(-1, 15, length = 100)))

# check the coefficient matrix of the ridge model
coef(model_ridge)

# use cross-validation to find the optimal lambda value for the ridge model
set.seed(1)
cv.ridge = cv.glmnet(x, y,
                     alpha = 0,
                     lambda = exp(seq(-1, 15, length = 100)),
                     type.measure = "mse")
par(mfrow = c(1, 1))
plot(cv.ridge)


# generate the trace plot
plot(model_ridge, xvar = "lambda", label = T)

# obtain the optimal lambda value
best.lambda = cv.ridge$lambda.min; best.lambda

# obtain the predicted coefficients
predict(model_ridge, s = best.lambda, type = "coefficients")
```

```{r lasso regression model}
# use cross-validation to find the optimal lambda value for lasso regression model
set.seed(1)
cv.lasso = cv.glmnet(x, y, alpha = 1, lambda = exp(seq(-1, 15, length = 100)))
cv.lasso$lambda.min

plot(cv.lasso)

# plot the result
plot(cv.lasso$glmnet.fit, xvar = "lambda", label = T)

# obtain the predicted coefficients
predict(cv.lasso, s = "lambda.min", type = "coefficients")
```

```{r}
ctrl1 = trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(1)
# fit a ridge model using caret package
ridge.fit = train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 0,
                                         lambda = exp(seq(-1, 15, length = 100))),
                  trControl = ctrl1)

# plot the RMSE by log(lambda)
plot(ridge.fit, xTrans = function(x) log(x))

# find the optimal lambda
ridge.fit$bestTune
```

